# -*- coding: utf-8 -*-
"""SJ-245A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JsI3Ww12sUQfMQeKNqcNtCkciqjBQsQ4

# Setting Up PySpark
"""

# Installing Java, Spark, and Findspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark

# Setting environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"

# Initializing Spark
import findspark
findspark.init()

"""# Initialize Spark Session"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import *
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression, RandomForestRegressor
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Creating Spark session
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("AirbnbAnalysis") \
    .getOrCreate()

"""# Data Loading and Initial Exploration

"""

# Loading the dataset with correct schema
df = spark.read.csv('/content/AB_NYC_2019.csv', header=True, inferSchema=True)

# Define correct schema for relevant columns
schema = df.schema
for field in schema:
    if field.name in ['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']:
        field.dataType = DoubleType()

# Load data with the defined schema, coercing errors
df = spark.read.csv('/content/AB_NYC_2019.csv', header=True, schema=schema, mode="DROPMALFORMED")

# Show schema
print("Schema:")
df.printSchema()

print("\nSample data:")
df.show(5)

"""# Data Cleaning and Preprocessing"""

# Checking for null values
print("Null values count:")
from pyspark.sql.functions import col
for col_name in df.columns:
    null_count = df.filter(col(col_name).isNull()).count()
    if null_count > 0:
        print(f"{col_name}: {null_count}")

# Droping rows with null values after attempting type conversion
df = df.na.drop()

print("\nAfter cleaning:")
df.show(5)

"""#Exploratory Data Analysis (EDA)"""

room_type_counts = df.groupBy("room_type").count().toPandas()

plt.figure(figsize=(8, 6))
plt.pie(room_type_counts['count'], labels=room_type_counts['room_type'], autopct='%1.1f%%')
plt.title('Room Type Distribution')
plt.show()

# Distribution of price
price_distribution = df.select("price").toPandas()

plt.figure(figsize=(10, 6))
sns.histplot(price_distribution['price'], bins=50, kde=True)
plt.title('Distribution of Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.xlim(0, 1000)
plt.show()



# Geographical distribution of listings
geo_distribution = df.select("latitude", "longitude").toPandas()

plt.figure(figsize=(10, 8))
sns.scatterplot(x='longitude', y='latitude', data=geo_distribution, alpha=0.5, s=5)
plt.title('Geographical Distribution of Listings in NYC')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

# Average price by neighbourhood group and room type
avg_price_neighbourhood_room = df.groupBy("neighbourhood_group", "room_type").agg(avg("price").alias("average_price")).toPandas()

plt.figure(figsize=(12, 6))
sns.barplot(x='neighbourhood_group', y='average_price', hue='room_type', data=avg_price_neighbourhood_room)
plt.title('Average Price by Neighbourhood Group and Room Type')
plt.xlabel('Neighbourhood Group')
plt.ylabel('Average Price')
plt.show()

# Correlation heatmap of numerical features
numerical_cols = ['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']
correlation_matrix = df.select(numerical_cols).toPandas().corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

"""# Feature Engineering and Outlier Removal"""

# Removing price outliers
df = df.filter(col("price") <= 500)

# Creating new features
df = df.withColumn("price_per_night", col("price") / col("minimum_nights"))
df = df.withColumn("host_experience",
                  when(col("number_of_reviews") > 100, "Experienced")
                  .when(col("number_of_reviews") > 50, "Intermediate")
                  .otherwise("New"))

# Showing new features
df.select("price", "minimum_nights", "price_per_night", "number_of_reviews", "host_experience").show(5)

"""# Prepare Data for Machine Learning"""

# Selecting features for modeling
features = ["neighbourhood_group", "room_type", "minimum_nights",
           "number_of_reviews", "reviews_per_month",
           "calculated_host_listings_count", "availability_365",
           "host_experience"]

# Defining stages for the pipeline
indexers = [StringIndexer(inputCol=col, outputCol=col+"_index", handleInvalid="keep")
            for col in ["neighbourhood_group", "room_type", "host_experience"]]

assembler = VectorAssembler(
    inputCols=[col+"_index" for col in ["neighbourhood_group", "room_type", "host_experience"]] +
    ["minimum_nights", "number_of_reviews", "reviews_per_month",
     "calculated_host_listings_count", "availability_365"],
    outputCol="features")

# Creating pipeline for feature transformation
pipeline = Pipeline(stages=indexers + [assembler])

# Fitting and transforming the data
transformed_data = pipeline.fit(df).transform(df)

# Splitting data into train and test sets
train_data, test_data = transformed_data.randomSplit([0.8, 0.2], seed=42)

print(f"Training data count: {train_data.count()}")
print(f"Test data count: {test_data.count()}")

"""# Linear Regression"""

# Initializing Linear Regression
lr = LinearRegression(featuresCol="features", labelCol="price")

# Training the model
lr_model = lr.fit(train_data)

# predictions
lr_predictions = lr_model.transform(test_data)

# Evaluating the model
lr_evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction")
lr_rmse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: "rmse"})
lr_r2 = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: "r2"})

print(f"Linear Regression RMSE: {lr_rmse}")
print(f"Linear Regression R2: {lr_r2}")

# Showing predictions vs actual
lr_predictions.select("price", "prediction").show(10)

"""# Random Forest Regression"""

# Initializing Random Forest Regressor
rf = RandomForestRegressor(featuresCol="features", labelCol="price")

# Training the model
rf_model = rf.fit(train_data)

# predictions
rf_predictions = rf_model.transform(test_data)

# Evaluating the model
rf_rmse = lr_evaluator.evaluate(rf_predictions, {lr_evaluator.metricName: "rmse"})
rf_r2 = lr_evaluator.evaluate(rf_predictions, {lr_evaluator.metricName: "r2"})

print(f"Random Forest RMSE: {rf_rmse}")
print(f"Random Forest R2: {rf_r2}")

# Feature importance
importances = rf_model.featureImportances
feature_names = ["neighbourhood_group_index", "room_type_index", "host_experience_index",
                "minimum_nights", "number_of_reviews", "reviews_per_month",
                "calculated_host_listings_count", "availability_365"]

importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances.toArray()
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance - Random Forest Regressor')
plt.show()

# Creating a comparison table
comparison_data = {
    'Model': ['Linear Regression', 'Random Forest Regression'],
    'RMSE': [f"{lr_rmse:.3f}", f"{rf_rmse:.3f}"],
    'R2': [f"{lr_r2:.3f}", f"{rf_r2:.3f}"]
}

comparison_df = pd.DataFrame(comparison_data)
print("\nModel Performance Comparison:")
print(comparison_df)